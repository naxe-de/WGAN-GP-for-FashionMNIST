{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1czVdIlqnImH"
      },
      "source": [
        "# WGAN-GP FashionMNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Project description\n",
        "The goal of this project was to train a Wasserstein GAN with Gradient Penalty (WGAN-GP) with the FashionMNIST repository by Zalando Research. The WGAN code is based on the coursera.org course on Generative Adverserial Networks by deeplearning.ai with some modifications to fit the purpose.\n",
        "\n",
        "###Project Repo\n",
        "\n",
        "https://github.com/naxe-de/WGAN-GP-for-FashionMNIST\n",
        "\n",
        "###Participants:\n",
        "\n",
        "Nils Axe\n",
        "\n",
        "Course and Semester\n",
        "\n",
        "Generative Adversarial Networks; WiSe2122"
      ],
      "metadata": {
        "id": "s1QBO1guPZrw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU8DDM6l9rZb"
      },
      "source": [
        "## Generator and Critic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sTpFE_eSk4s"
      },
      "source": [
        "#### Packages and Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfkorNJrnmNO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from tqdm.auto import tqdm\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.datasets import FashionMNIST\n",
        "import torchvision.models as models\n",
        "\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "torch.manual_seed(0)\n",
        "\n",
        "def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)):\n",
        "    '''\n",
        "    Function for visualizing images: Given a tensor of images, number of images, and\n",
        "    size per image, plots and prints the images in an uniform grid.\n",
        "    '''\n",
        "    image_tensor = (image_tensor + 1) / 2\n",
        "    image_unflat = image_tensor.detach().cpu()\n",
        "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
        "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
        "    plt.show()\n",
        "\n",
        "def make_grad_hook():\n",
        "    '''\n",
        "    Function to keep track of gradients for visualization purposes, \n",
        "    which fills the grads list when using model.apply(grad_hook).\n",
        "    '''\n",
        "    grads = []\n",
        "    def grad_hook(m):\n",
        "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "            grads.append(m.weight.grad)\n",
        "    return grads, grad_hook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1A1M6kpnfxw"
      },
      "source": [
        "#### Generator and Noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFwajQ3tGgI2"
      },
      "outputs": [],
      "source": [
        "##  Generator:\n",
        "#   Values:\n",
        "#       z_dim: the dimension of the noise vector, a scalar\n",
        "#       im_chan: number of channels in the images, fitted for the dataset used, a scalar\n",
        "#              (MNIST is black-and-white, so 1 channel is your default) ## 3 for color RGB\n",
        "#       hidden_dim: the inner dimension, a scalar\n",
        "#   Parameters:\n",
        "#       input_channels: how many channels the input feature representation has\n",
        "#       output_channels: how many channels the output feature representation should have\n",
        "#       kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
        "#       stride: the stride of the convolution\n",
        "#       final_layer: a boolean, true if it is the final layer and false otherwise \n",
        "#                  (affects activation and batchnorm)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, z_dim=10, im_chan=1, hidden_dim=64):\n",
        "        super(Generator, self).__init__()\n",
        "        self.z_dim = z_dim\n",
        "        # Build the neural network\n",
        "        self.gen = nn.Sequential(\n",
        "            self.make_gen_block(z_dim, hidden_dim * 4),\n",
        "            self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1),\n",
        "            self.make_gen_block(hidden_dim * 2, hidden_dim),\n",
        "            self.make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True),\n",
        "        )\n",
        "\n",
        "    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n",
        "\n",
        " #       Function to return a sequence of operations corresponding to a generator block of DCGAN;\n",
        "\n",
        "        if not final_layer:\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),#Convolution\n",
        "                nn.BatchNorm2d(output_channels),#batchnorm\n",
        "                nn.ReLU(inplace=True),#activation\n",
        "            )\n",
        "        else:\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
        "                nn.Tanh(),\n",
        "            )\n",
        "\n",
        "    def forward(self, noise):\n",
        "        '''\n",
        "        Function for completing a forward pass of the generator: Given a noise tensor,\n",
        "        returns generated images.\n",
        "        Parameters:\n",
        "            noise: a noise tensor with dimensions (n_samples, z_dim)\n",
        "        '''\n",
        "        x = noise.view(len(noise), self.z_dim, 1, 1)\n",
        "        return self.gen(x)\n",
        "\n",
        "def get_noise(n_samples, z_dim, device='cpu'):\n",
        "    '''\n",
        "    Function for creating noise vectors: Given the dimensions (n_samples, z_dim)\n",
        "    creates a tensor of that shape filled with random numbers from the normal distribution.\n",
        "    Parameters:\n",
        "      n_samples: the number of samples to generate, a scalar\n",
        "      z_dim: the dimension of the noise vector, a scalar\n",
        "      device: the device type\n",
        "    '''\n",
        "    return torch.randn(n_samples, z_dim, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9fScH98nkYH"
      },
      "source": [
        "#### Critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aA4AxGnmpuPq"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "    '''\n",
        "    Critic Class\n",
        "    Values:\n",
        "        im_chan: the number of channels in the images, fitted for the dataset used, a scalar\n",
        "              (MNIST is black-and-white, so 1 channel is your default) ### 3 for RGB Color\n",
        "        hidden_dim: the inner dimension, a scalar\n",
        "    '''\n",
        "    def __init__(self, im_chan=1, hidden_dim=64):\n",
        "        super(Critic, self).__init__()\n",
        "        self.crit = nn.Sequential(\n",
        "            self.make_crit_block(im_chan, hidden_dim),\n",
        "            self.make_crit_block(hidden_dim, hidden_dim * 2),\n",
        "            self.make_crit_block(hidden_dim * 2, 1, final_layer=True),\n",
        "        )\n",
        "\n",
        "    def make_crit_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n",
        "        '''\n",
        "        Function to return a sequence of operations corresponding to a critic block of DCGAN;\n",
        "        a convolution, a batchnorm (except in the final layer), and an activation (except in the final layer).\n",
        "        Parameters:\n",
        "            input_channels: how many channels the input feature representation has\n",
        "            output_channels: how many channels the output feature representation should have\n",
        "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
        "            stride: the stride of the convolution\n",
        "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
        "                      (affects activation and batchnorm)\n",
        "        '''\n",
        "        if not final_layer:\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
        "                nn.BatchNorm2d(output_channels),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "            )\n",
        "        else:\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
        "            )\n",
        "\n",
        "    def forward(self, image):\n",
        "        '''\n",
        "        Function for completing a forward pass of the critic: Given an image tensor, \n",
        "        returns a 1-dimension tensor representing fake/real.\n",
        "        Parameters:\n",
        "            image: a flattened image tensor with dimension (im_chan)\n",
        "        '''\n",
        "        crit_pred = self.crit(image)\n",
        "        return crit_pred.view(len(crit_pred), -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRk_8azSq3tF"
      },
      "source": [
        "## Training Initializations\n",
        "Now you can start putting it all together.\n",
        "As usual, you will start by setting the parameters:\n",
        "  *   n_epochs: the number of times you iterate through the entire dataset when training\n",
        "  *   z_dim: the dimension of the noise vector\n",
        "  *   display_step: how often to display/visualize the images\n",
        "  *   batch_size: the number of images per forward/backward pass\n",
        "  *   lr: the learning rate\n",
        "  *   beta_1, beta_2: the momentum terms\n",
        "  *   c_lambda: weight of the gradient penalty\n",
        "  *   crit_repeats: number of times to update the critic per generator update - there are more details about this in the *Putting It All Together* section\n",
        "  *   device: the device type\n",
        "\n",
        "You will also load and transform the MNIST dataset to tensors.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFLQ039u-qdu"
      },
      "outputs": [],
      "source": [
        "n_epochs = 50\n",
        "z_dim = 64\n",
        "display_step = 50\n",
        "batch_size = 128\n",
        "lr = 0.0002\n",
        "beta_1 = 0.5\n",
        "beta_2 = 0.999\n",
        "c_lambda = 10\n",
        "crit_repeats = 5\n",
        "device = 'cuda'\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,)), #channel adden fÃ¼r rgb\n",
        "])\n",
        "## Dataset laden (hier aus https://pytorch.org/vision/stable/datasets.html)\n",
        "\n",
        "#FashionMNIST\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    FashionMNIST('.', train = 1, download=True, transform=transform, ),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24Var22i_Ccs"
      },
      "source": [
        "####Initializing generator, critic, and optimizers.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDFRZ8tg_Y57"
      },
      "outputs": [],
      "source": [
        "gen = Generator(z_dim).to(device)\n",
        "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
        "crit = Critic().to(device) \n",
        "crit_opt = torch.optim.Adam(crit.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "    if isinstance(m, nn.BatchNorm2d):\n",
        "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias, 0)\n",
        "gen = gen.apply(weights_init)\n",
        "crit = crit.apply(weights_init)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFEi5BhVX5-P"
      },
      "source": [
        "## Gradient Penalty\n",
        "\n",
        "1.   compute the gradient with respect to the images \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn4dkXnNtcv6"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Parameters:\n",
        "        crit: the critic model\n",
        "        real: a batch of real images\n",
        "        fake: a batch of fake images\n",
        "        epsilon: a vector of the uniformly random proportions of real/fake per mixed image\n",
        "    Returns:\n",
        "        gradient: the gradient of the critic's scores, with respect to the mixed image\n",
        "'''\n",
        "\n",
        "def get_gradient(crit, real, fake, epsilon):\n",
        "\n",
        "    # Mix the images together\n",
        "    mixed_images = real * epsilon + fake * (1 - epsilon)\n",
        "\n",
        "    # Calculate the critic's scores on the mixed images\n",
        "    mixed_scores = crit(mixed_images)\n",
        "    \n",
        "    # Take the gradient of the scores with respect to the images\n",
        "    gradient = torch.autograd.grad(\n",
        "\n",
        "        # https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad\n",
        "        inputs=mixed_images,\n",
        "        outputs=mixed_scores,\n",
        "        \n",
        "        # These other parameters have to do with the pytorch autograd engine works\n",
        "        grad_outputs=torch.ones_like(mixed_scores), \n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "    )[0]\n",
        "    return gradient\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5PMRrMpRUK-"
      },
      "source": [
        "2.   compute the gradient penalty given the gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPwBH83IzCpS"
      },
      "outputs": [],
      "source": [
        "#    Return the gradient penalty, given a gradient.\n",
        "#    Given a batch of image gradients, you calculate the magnitude of each image's gradient\n",
        "#    and penalize the mean quadratic distance of each magnitude to 1.\n",
        "#    Parameters:\n",
        "#        gradient: the gradient of the critic's scores, with respect to the mixed image\n",
        "#    Returns:\n",
        "#        penalty: the gradient penalty\n",
        "\n",
        "def gradient_penalty(gradient):\n",
        "\n",
        "    # Flatten the gradients so that each row captures one image\n",
        "    gradient = gradient.view(len(gradient), -1)\n",
        "\n",
        "    # Calculate the magnitude of every row\n",
        "    gradient_norm = gradient.norm(2, dim=1)\n",
        "    \n",
        "    # Penalize the mean squared distance of the gradient norms from 1\n",
        "    penalty = torch.mean((gradient_norm - 1)**2)\n",
        "\n",
        "    return penalty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sob-u9Z1X9sb"
      },
      "source": [
        "## Losses\n",
        "calculate the loss for the generator and the critic."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Generator Loss"
      ],
      "metadata": {
        "id": "M1jPSXYpmM7A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnJFs-qkMCA-"
      },
      "outputs": [],
      "source": [
        "\n",
        "#    Return the loss of a generator given the critic's scores of the generator's fake images.\n",
        "#    Parameters:\n",
        "#        crit_fake_pred: the critic's scores of the fake images\n",
        "#    Returns:\n",
        "#        gen_loss: a scalar loss value for the current batch of the generator\n",
        "\n",
        "def get_gen_loss(crit_fake_pred):\n",
        "    gen_loss = -torch.mean(crit_fake_pred)\n",
        "    return gen_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt3MgH68TM1P"
      },
      "source": [
        "### Critic Loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jvbz1zDMTdu"
      },
      "outputs": [],
      "source": [
        "#    Return the loss of a critic given the critic's scores for fake and real images,\n",
        "#    the gradient penalty, and gradient penalty weight.\n",
        "#    Parameters:\n",
        "#        crit_fake_pred: the critic's scores of the fake images\n",
        "#        crit_real_pred: the critic's scores of the real images\n",
        "#        gp: the unweighted gradient penalty\n",
        "#        c_lambda: the current weight of the gradient penalty \n",
        "#    Returns:\n",
        "#        crit_loss: a scalar for the critic's loss, accounting for the relevant factors\n",
        "\n",
        "def get_crit_loss(crit_fake_pred, crit_real_pred, gp, c_lambda):\n",
        "    crit_loss = torch.mean(crit_fake_pred) - torch.mean(crit_real_pred) + c_lambda * gp\n",
        "    return crit_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x5wu7rUMlnZ"
      },
      "source": [
        "## Training the GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXptQZcwrBrq"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cur_step = 0\n",
        "generator_losses = []\n",
        "critic_losses = []\n",
        "for epoch in range(n_epochs):\n",
        "    # Dataloader returns the batches\n",
        "    for real, _ in tqdm(dataloader):\n",
        "        cur_batch_size = len(real)\n",
        "        real = real.to(device)\n",
        "\n",
        "        mean_iteration_critic_loss = 0\n",
        "        for _ in range(crit_repeats):\n",
        "            ### Update critic ###\n",
        "            crit_opt.zero_grad()\n",
        "            fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n",
        "            fake = gen(fake_noise)\n",
        "            crit_fake_pred = crit(fake.detach())\n",
        "            crit_real_pred = crit(real)\n",
        "\n",
        "            epsilon = torch.rand(len(real), 1, 1, 1, device=device, requires_grad=True)\n",
        "            gradient = get_gradient(crit, real, fake.detach(), epsilon)\n",
        "            gp = gradient_penalty(gradient)\n",
        "            crit_loss = get_crit_loss(crit_fake_pred, crit_real_pred, gp, c_lambda)\n",
        "\n",
        "            # Keep track of the average critic loss in this batch\n",
        "            mean_iteration_critic_loss += crit_loss.item() / crit_repeats\n",
        "            # Update gradients\n",
        "            crit_loss.backward(retain_graph=True)\n",
        "            # Update optimizer\n",
        "            crit_opt.step()\n",
        "        critic_losses += [mean_iteration_critic_loss]\n",
        "\n",
        "        ### Update generator ###\n",
        "        gen_opt.zero_grad()\n",
        "        fake_noise_2 = get_noise(cur_batch_size, z_dim, device=device)\n",
        "        fake_2 = gen(fake_noise_2)\n",
        "        crit_fake_pred = crit(fake_2)\n",
        "        \n",
        "        gen_loss = get_gen_loss(crit_fake_pred)\n",
        "        gen_loss.backward()\n",
        "\n",
        "        # Update the weights\n",
        "        gen_opt.step()\n",
        "\n",
        "        # Keep track of the average generator loss\n",
        "        generator_losses += [gen_loss.item()]\n",
        "\n",
        "        ### Visualization code ###\n",
        "        if cur_step % display_step == 0 and cur_step > 0:\n",
        "            gen_mean = sum(generator_losses[-display_step:]) / display_step\n",
        "            crit_mean = sum(critic_losses[-display_step:]) / display_step\n",
        "            print(f\"Step {cur_step}: Generator loss: {gen_mean}, critic loss: {crit_mean}\")\n",
        "            show_tensor_images(fake)\n",
        "            show_tensor_images(real)\n",
        "            step_bins = 20\n",
        "            num_examples = (len(generator_losses) // step_bins) * step_bins\n",
        "            plt.plot(\n",
        "                range(num_examples // step_bins), \n",
        "                torch.Tensor(generator_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
        "                label=\"Generator Loss\"\n",
        "            )\n",
        "            plt.plot(\n",
        "                range(num_examples // step_bins), \n",
        "                torch.Tensor(critic_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
        "                label=\"Critic Loss\"\n",
        "            )\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "        cur_step += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving and loading progress"
      ],
      "metadata": {
        "id": "UoGpRtm4l1eX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting GDrive\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "os.listdir('/content/drive/MyDrive/OC_GAN_FashionMNIST/')"
      ],
      "metadata": {
        "id": "yv3ge6E9mDTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0r6GXfc8_SEP"
      },
      "outputs": [],
      "source": [
        "# Saving progress\n",
        "#Generator\n",
        "torch.save(gen.state_dict(), '/content/drive/MyDrive/OC_GAN_FashionMNIST/generator_weights.pth')\n",
        "torch.save(gen_opt.state_dict(), '/content/drive/MyDrive/OC_GAN_FashionMNIST/generator_opt.pth') \n",
        "#Critic\n",
        "torch.save(crit.state_dict(), '/content/drive/MyDrive/OC_GAN_FashionMNIST/critic_weights.pth')\n",
        "torch.save(crit_opt.state_dict(), '/content/drive/MyDrive/OC_GAN_FashionMNIST/critic_opt.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsrCJH5D_SER"
      },
      "outputs": [],
      "source": [
        "# Loading progress\n",
        "#Generator\n",
        "gen = Generator(z_dim).to(device)\n",
        "gen.load_state_dict(torch.load('/content/drive/MyDrive/OC_GAN_FashionMNIST/generator_weights.pth'))\n",
        "gen.train\n",
        "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
        "gen_opt.load_state_dict(torch.load('/content/drive/MyDrive/OC_GAN_FashionMNIST/generator_opt.pth'))\n",
        "\n",
        "#Critic\n",
        "crit = Critic().to(device)\n",
        "crit.load_state_dict(torch.load('/content/drive/MyDrive/OC_GAN_FashionMNIST/critic_weights.pth'))\n",
        "crit.train\n",
        "crit_opt = torch.optim.Adam(crit.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
        "crit_opt.load_state_dict(torch.load('/content/drive/MyDrive/OC_GAN_FashionMNIST/critic_opt.pth'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "WGAN-GP for FashionMNIST.ipynb",
      "provenance": []
    },
    "coursera": {
      "schema_names": [
        "GANSC1-3A"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}